# TicTacToe RL
使用强化学习玩井字棋！  

## 试玩
python play_tictactoe_qlearning.py  
python play_tictactoe_ddqn.py  

## 算法介绍
Q-Learning：  
1、将棋盘board哈希成编号（0 - 19682），创建一个19683 * 9的Q表格，表示在board情况下走action后可获得的回报期望；  
2、创建一个19683 * 9的动作次数表，记录在board情况下选择走action的次数；  
3、创建两个AI玩家，其中一个永远选择Q值最大的action，另一个有epsilon概率选择尝试次数最少的action，其他情况同样选择Q值最大的action，epsilon可以随着游戏轮数从1下降到0.2；  
4、为保证两个AI的策略差异不会影响到Q表更新方式，我将棋盘翻转（o和x互换）后再交给另一个AI去选择action，保证AI始终站在同一方向去训练策略；  
5、根据Q-Learning更新规则更新Q表，不过在井字棋中Q目标要取负号，因为对手会选择Q值最大的action但对于我方是Q值最小的；  
6、在随机先手情况下循环训练50000场游戏，大约在30000场时1号AI就已经处于不败地位，训练结束。

DQN：  
1、将棋盘改为独热格式，即3 * 3 * 3；  
2、搭建DQN网络，创建两个AI玩家，使用同一个网络，一个永远选择Q值最大的action，另一个有epsilon概率随机选择，其余情况同样选择Q值最大的action，epsilon随游戏轮数从1下降到0.2；  
3、和Q-Learning一样，将棋盘翻转后再交给另一个AI进行选择；  
4、创建记忆库，记录游戏每一步，并赋予每一条记忆一个初始权重，用于采样；  
5、在记忆库有一定量数据后，每一轮都从记忆库中采样训练，并把每一个样本的训练误差赋值到该样本的采样权重中（误差越大，抽到概率越高）；  
6、模型更新采取double dqn方式更新，即Q目标的动作由评价网络决定，但Q值由目标网络计算得到；  
7、DQN网络比较难收敛，需要不断修改学习率、回报折扣率、目标网络更新频率、甚至网络结构才能训练出一个收敛的结果，目前测试与0.2随机策略下的网络自己比赛，取得20000场不败记录，可以认为是已经得到最佳网络。